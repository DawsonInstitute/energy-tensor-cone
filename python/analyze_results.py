import json
import datetime
from pathlib import Path


def fourier_inverse_square_benchmark(g_samples: list[float], dt: float) -> float:
    """Toy Fourier-space benchmark integral with an inverse-square weight.

    This is *not* a full implementation of a model-specific QEI bound, which
    requires additional state/field data. It exists to support numerical
    sanity-checks and figure generation that compare proxy bounds against a
    representative inverse-frequency weighting.

    Returns a nonnegative scalar.
    """

    if dt <= 0:
        raise ValueError("dt must be positive")
    if not g_samples:
        raise ValueError("g_samples must be non-empty")

    try:
        import numpy as np
    except Exception as e:  # pragma: no cover
        raise RuntimeError("numpy is required for Fourier benchmark") from e

    g = np.asarray(g_samples, dtype=float)
    n = int(g.shape[0])
    freqs = np.fft.fftfreq(n, d=float(dt))
    g_hat = np.fft.fft(g)

    eps = 1e-12
    weight = 1.0 / (freqs * freqs + eps)
    # Discrete sum as a rough stand-in for a continuum integral.
    val = (1.0 / (2.0 * np.pi)) * float(np.sum(np.abs(g_hat) ** 2 * weight))
    return max(0.0, val)


def _as_lean_string(s: str) -> str:
    return '"' + s.replace('\\', '\\\\').replace('"', '\\"') + '"'


def _as_lean_float(x) -> str:
    # Lean's Float literals accept standard decimal / scientific notation.
    if isinstance(x, (int, float)):
        return repr(float(x))
    return repr(float(x))


def generate_lean_candidates(results_dir: Path, out_file: Path, top_k: int = 5) -> None:
    """Generate a Lean documentation file for candidate vertex data.

    Reads vertex.json (the certified LP vertex) and top_near_misses.json (if
    available) from results_dir and produces a self-contained Lean comment block
    that records the candidate data for human inspection.  The formal
    certification itself lives in AQEI_Generated_Data_Rat.lean; this file serves
    as a human-readable companion for the pipeline audit trail.
    """
    results_dir = Path(results_dir)
    out_file = Path(out_file)
    out_file.parent.mkdir(parents=True, exist_ok=True)

    lines: list[str] = []
    lines.append("import Std")
    lines.append("")
    lines.append("/- Auto-generated by python/analyze_results.py.")
    lines.append("   Formal certification uses AQEI_Generated_Data_Rat.lean. -/")
    lines.append("")

    # Try to load the certified vertex from vertex.json.
    vertex_path = results_dir / "vertex.json"
    if vertex_path.exists():
        data = json.loads(vertex_path.read_text())
        a = data.get("a", [])
        active_indices = data.get("activeIndices", [])
        constraints = data.get("constraints", [])

        lines.append("/- Certified LP vertex (floating-point):")
        lines.append(f"   Coefficients a = {a}")
        lines.append(f"   Active AQEI constraint indices: {active_indices}")
        lines.append(f"   Number of active constraints: {len(constraints)} AQEI + box")
        lines.append("   See AQEI_Generated_Data_Rat.lean for the exact rational form")
        lines.append("   used in the Lean formal proof (FinalTheorems.lean).")
        lines.append("-/")
        lines.append("")

        # Emit one Lean Float constant per active constraint bound for cross-checking.
        lines.append("namespace GeneratedCandidates")
        lines.append("")
        lines.append("/-- Active-constraint B values (proxy bounds B_model) from vertex.json --/")
        for k, c in enumerate(constraints[:top_k]):
            b_val = c.get("B", 0.0)
            lines.append(f"def active_B_{k} : Float := {_as_lean_float(b_val)}")
        lines.append("")
        lines.append("end GeneratedCandidates")
    else:
        # No vertex data available yet; write a minimal placeholder.
        lines.append("/- No vertex.json found in results_dir at generation time.")
        lines.append("   Run the Mathematica search (mathematica/search.m) to produce it. -/")

    # Try to load near-miss candidates for the audit log.
    top_path = results_dir / "top_near_misses.json"
    if top_path.exists():
        near_miss_data = json.loads(top_path.read_text())[:top_k]
        lines.append("")
        lines.append(f"/- {len(near_miss_data)} near-miss candidate(s) from top_near_misses.json also")
        lines.append("   available for independent analysis. -/")

    lines.append("")
    out_file.write_text("\n".join(lines) + "\n")
    print(f"Wrote {out_file}")


def export_pipeline_validation(results_dir: Path) -> None:
    """Validate the LP vertex and export an audit artifact (pipeline_validation.json).

    Reads vertex.json, checks that every active AQEI constraint is saturated
    (L·a + B ≈ 0), and writes a summary to pipeline_validation.json.  This
    creates a machine-readable record of the pipeline's integrity for downstream
    independent re-checking as described in the paper's Computational Methodology
    section.
    """
    results_dir = Path(results_dir)
    vertex_path = results_dir / "vertex.json"
    if not vertex_path.exists():
        print("pipeline_validation: vertex.json not found, skipping validation export")
        return

    data = json.loads(vertex_path.read_text())
    a = data.get("a", [])
    active_indices = data.get("activeIndices", [])
    constraints = data.get("constraints", [])

    saturation_records = []
    all_tight = True
    for idx, c in zip(active_indices, constraints):
        L = c.get("L", [])
        B = float(c.get("B", 0.0))
        La = sum(float(li) * float(ai) for li, ai in zip(L, a))
        residual = abs(La + B)
        tight = residual < 1e-6
        if not tight:
            all_tight = False
        saturation_records.append({
            "constraint_index": idx,
            "La_plus_B": La + B,
            "abs_residual": residual,
            "tight": tight,
        })

    validation = {
        "timestamp": datetime.datetime.utcnow().isoformat() + "Z",
        "vertex_source": str(vertex_path),
        "num_basis": data.get("numBasis", len(a)),
        "num_active_aqei_constraints": len(constraints),
        "all_constraints_tight": all_tight,
        "tolerance_threshold": 1e-6,
        "saturation_residuals": saturation_records,
    }

    out_path = results_dir / "pipeline_validation.json"
    out_path.write_text(json.dumps(validation, indent=2))

    status = "PASS" if all_tight else "FAIL"
    print(f"Pipeline validation [{status}]: {len(constraints)} active constraints, "
          f"max residual = {max(r['abs_residual'] for r in saturation_records):.3e}")
    print(f"  Exported audit record to {out_path}")


def analyze_results(results_dir: Path = None) -> None:
    """Analyze and report on JSON results from Mathematica search."""
    if results_dir is None:
        results_dir = Path(__file__).parent.parent / "mathematica" / "results"

    results_dir = Path(results_dir)

    # Report on available result files.
    for fname in ["summary.json", "near_misses.json", "top_near_misses.json", "vertex.json"]:
        fpath = results_dir / fname
        if fpath.exists():
            print(f"  Found: {fname}")

    # Export pipeline validation artifact.
    export_pipeline_validation(results_dir)


if __name__ == "__main__":
    analyze_results()
